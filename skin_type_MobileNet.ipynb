{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Type Model\n",
    "This model is the model that used to classify user's skin type and have an output of multi-class from 3 possible class (oily,normal,dry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries, Mobilenet, and Env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Env File\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "datasets_path = os.getenv('DATASET_PATH_TYPE_2')\n",
    "# datasets_path = os.getenv('DATASET_PATH_TYPE_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing MobileNetV2 RestNet With ImagNet Weight Without The Top Layer\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membekukan semua lapisan dari model MobileNetV2\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning: Membuka beberapa lapisan terakhir dari MobileNetV2\n",
    "for layer in base_model.layers[-5:]:  # Mengatur lebih banyak lapisan terakhir dapat dilatih\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Model (If Exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load model from .h5 file\n",
    "model = load_model('D:/capstone_models/skin_type_model_2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "class SkinTypeDataset(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"DatasetBuilder for skin type detection.\"\"\"\n",
    "    VERSION = tfds.core.Version('1.0.0')\n",
    "    MANUAL_DOWNLOAD_INSTRUCTIONS = \"Please ensure the skin type dataset is downloaded and located at the right path (look at env file)\"\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            description=(\"Dataset for skin type classification with 3 labels: oily, normal, dry\"),\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                'image': tfds.features.Image(shape=(224, 224, 3)),\n",
    "                'label': tfds.features.Tensor(shape=(3,), dtype=tf.float32),\n",
    "            }),\n",
    "            supervised_keys=('image', 'label'),\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        path = dl_manager.manual_dir\n",
    "        # Read all images and labels\n",
    "        all_data = []\n",
    "        for category in ['oily', 'normal', 'dry']:\n",
    "            category_path = os.path.join(path, category)\n",
    "            for filename in os.listdir(category_path):\n",
    "                all_data.append((filename, category))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(all_data, columns=['filename', 'label'])\n",
    "        \n",
    "        # Split data into train, val, and test\n",
    "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 of train for validation\n",
    "        \n",
    "        return {\n",
    "            'train': self._generate_examples(train_df, path),\n",
    "            'val': self._generate_examples(val_df, path),\n",
    "            'test': self._generate_examples(test_df, path),\n",
    "        }\n",
    "\n",
    "    def _generate_examples(self, dataframe, base_path):\n",
    "        label_map = {\n",
    "            'oily': [1, 0, 0],\n",
    "            'normal': [0, 1, 0],\n",
    "            'dry': [0, 0, 1]\n",
    "        }\n",
    "        # Load images and their labels\n",
    "        for _, row in dataframe.iterrows():\n",
    "            image_path = os.path.join(base_path, row['label'], row['filename'])\n",
    "            image = tf.io.read_file(image_path)  # Read image file\n",
    "            image = tf.io.decode_image(image, channels=3)  # Decode image to tensor\n",
    "            image = tf.image.resize(image, (224, 224))  # Resize image\n",
    "            image = tf.cast(image, tf.uint8)  # Convert to uint8\n",
    "            label = tf.cast(label_map[row['label']], tf.float32)  # Cast to float32\n",
    "            yield row['filename'], {  # Use filename as the unique key\n",
    "                'image': image.numpy(),\n",
    "                'label': label.numpy(),\n",
    "            }\n",
    "\n",
    "# Use the updated dataset class\n",
    "builder = SkinTypeDataset(data_dir=datasets_path)\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset dalam bentuk builder untuk melakukan cek\n",
    "ds_train = builder.as_dataset(split='train')\n",
    "ds_val = builder.as_dataset(split='val')\n",
    "ds_test = builder.as_dataset(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dan tampilkan label dengan nilai [0, 0, 1]\n",
    "for i, example in enumerate(ds_train):\n",
    "    label = example['label'].numpy()  # Ambil label sebagai numpy array\n",
    "    if (label == [0, 0, 1]).all():    \n",
    "        print(f\"Label {i + 1}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fungsi untuk menampilkan gambar\n",
    "def show_image(image):\n",
    "    # Konversi gambar tensor ke numpy array dan tampilkan dengan matplotlib\n",
    "    image = image.numpy()  # Ubah tensor menjadi numpy array\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Nonaktifkan axis\n",
    "    plt.show()\n",
    "\n",
    "# Fungsi untuk menampilkan beberapa gambar\n",
    "def show_images_from_dataset(dataset, num_images=5):\n",
    "    for i, data in enumerate(dataset.take(num_images)):  # Ambil beberapa gambar pertama dari dataset\n",
    "        image = data['image']\n",
    "        # Tampilkan gambar\n",
    "        show_image(image)\n",
    "\n",
    "# Menampilkan gambar pertama dari ds_train\n",
    "show_images_from_dataset(ds_train, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Dataset And Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset yang sudah diproses\n",
    "ds_train = tfds.load('skin_type_dataset', split='train', data_dir=datasets_path)\n",
    "ds_val = tfds.load('skin_type_dataset', split='val', data_dir=datasets_path)\n",
    "ds_test = tfds.load('skin_type_dataset', split='test', data_dir=datasets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    \n",
    "    # Menambahkan random zoom\n",
    "    scale = tf.random.uniform(shape=[], minval=0.8, maxval=1.2, dtype=tf.float32)\n",
    "    new_height = tf.cast(scale * 224, tf.int32)\n",
    "    new_width = tf.cast(scale * 224, tf.int32)\n",
    "    image = tf.image.resize(image, (new_height, new_width))\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 224, 224)  # Mengembalikan ke ukuran 224x224\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Function to ensure data is paired as (image, label)\n",
    "def preprocess(data):\n",
    "    image = data['image']\n",
    "    label = data['label']\n",
    "    image = preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing and augmentation\n",
    "ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.map(augment_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(augment_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(augment_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Batch and prefetch the datasets\n",
    "ds_train = ds_train.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.batch(16).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(16).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung jumlah data dalam ds_train\n",
    "num_samples = sum(1 for _ in ds_train)\n",
    "# num_samples = sum(1 for _ in ds_train.unbatch())\n",
    "\n",
    "print(f\"Jumlah gambar dan label dalam ds_train: {num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, BatchNormalization, GlobalAveragePooling2D, Dense, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Menambahkan lapisan kustom di atas MobileNetV2\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) \n",
    "x = Dense(256, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)  \n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(3, activation='softmax', kernel_regularizer=l2(0.01))(x)  # Menggunakan softmax untuk multi-class classification\n",
    "\n",
    "# Membuat model akhir\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "# # Menyesuaikan optimizer dan learning rate \n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9) \n",
    "# # Compiling the model \n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Model Architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in ds_train.take(1):\n",
    "    print(image.shape, label.shape)  # Pastikan gambar memiliki shape ((batch size), 224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Callback untuk menghentikan pelatihan jika validasi loss tidak membaik\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',                  \n",
    "    patience=5,                         \n",
    "    restore_best_weights=True,           \n",
    "    verbose=1                            \n",
    ")\n",
    "\n",
    "# Callback untuk mengurangi learning rate jika validasi loss stagnan\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',                \n",
    "    factor=0.5,                         \n",
    "    patience=3,                         \n",
    "    verbose=1                           \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = {0: 0.6 , 1: 1.2 , 2: 0.6}  # Sesuaikan bobot berdasarkan kinerja kelas\n",
    "\n",
    "# Melatih model dan mencatat hasil pelatihan dalam objek `history`\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=30,\n",
    "    # class_weight=class_weight,\n",
    "    callbacks=[early_stopping, lr_scheduler] \n",
    ")\n",
    "\n",
    "# Menampilkan metrik dengan matplotlib\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(accuracy))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluasi model pada data testing\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f'Testing Loss: {loss}, Testing Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil nilai akurasi pada epoch terbaik\n",
    "best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "\n",
    "train_accuracy = history.history['accuracy'][best_epoch] \n",
    "val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "\n",
    "# Akurasi data testing (sudah didapat dari evaluasi model sebelumnya)\n",
    "test_accuracy = accuracy\n",
    "\n",
    "# Buat bar chart\n",
    "labels = ['Training Accuracy', 'Validation Accuracy', 'Testing Accuracy']\n",
    "accuracies = [train_accuracy, val_accuracy, test_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, accuracies, color=['blue', 'orange', 'green'])\n",
    "plt.ylim(0, 1)  # Atur batas y dari 0 sampai 1\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat prediksi pada data testing\n",
    "y_pred = model.predict(ds_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Mendapatkan label aktual\n",
    "y_true = np.concatenate([y for x, y in ds_test], axis=0)\n",
    "y_true_classes = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Menghitung confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Menampilkan confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Oily', 'Normal', 'Dry'], yticklabels=['Oily', 'Normal', 'Dry'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Menampilkan laporan klasifikasi\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=['Oily', 'Normal', 'Dry']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in .h5 format\n",
    "model.save('D:/capstone_models/skin_type_model_temp.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load model dari file .keras\n",
    "model = tf.keras.models.load_model('D:/capstone_models/skin_type_model.keras')\n",
    "\n",
    "# Buat converter untuk mengonversi model ke TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Simpan model TFLite ke file\n",
    "with open('D:/capstone_models/skin_type_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Lanjutan (Rawan Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = {0: 0.6 , 1: 1.2 , 2: 0.6}  # Sesuaikan bobot berdasarkan kinerja kelas\n",
    "\n",
    "# Melatih model dan mencatat hasil pelatihan dalam objek `history`\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=10,\n",
    "    # class_weight=class_weight,\n",
    "    callbacks=[early_stopping, lr_scheduler] \n",
    ")\n",
    "\n",
    "# Menampilkan metrik dengan matplotlib\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(accuracy))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluasi model pada data testing\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f'Testing Loss: {loss}, Testing Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat prediksi pada data testing\n",
    "y_pred = model.predict(ds_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Mendapatkan label aktual\n",
    "y_true = np.concatenate([y for x, y in ds_test], axis=0)\n",
    "y_true_classes = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Menghitung confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Menampilkan confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Oily', 'Normal', 'Dry'], yticklabels=['Oily', 'Normal', 'Dry'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Menampilkan laporan klasifikasi\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=['Oily', 'Normal', 'Dry']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = {0: 0.6 , 1: 1.2 , 2: 0.6}  # Sesuaikan bobot berdasarkan kinerja kelas\n",
    "\n",
    "# Melatih model dan mencatat hasil pelatihan dalam objek `history`\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=30,\n",
    "    # class_weight=class_weight,\n",
    "    callbacks=[early_stopping, lr_scheduler] \n",
    ")\n",
    "\n",
    "# Menampilkan metrik dengan matplotlib\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(accuracy))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluasi model pada data testing\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f'Testing Loss: {loss}, Testing Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil nilai akurasi pada epoch terbaik\n",
    "best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "\n",
    "train_accuracy = history.history['accuracy'][best_epoch] \n",
    "val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "\n",
    "# Akurasi data testing (sudah didapat dari evaluasi model sebelumnya)\n",
    "test_accuracy = accuracy\n",
    "\n",
    "# Buat bar chart\n",
    "labels = ['Training Accuracy', 'Validation Accuracy', 'Testing Accuracy']\n",
    "accuracies = [train_accuracy, val_accuracy, test_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, accuracies, color=['blue', 'orange', 'green'])\n",
    "plt.ylim(0, 1)  # Atur batas y dari 0 sampai 1\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat prediksi pada data testing\n",
    "y_pred = model.predict(ds_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Mendapatkan label aktual\n",
    "y_true = np.concatenate([y for x, y in ds_test], axis=0)\n",
    "y_true_classes = np.argmax(y_true, axis=1)\n",
    "\n",
    "# Menghitung confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Menampilkan confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Oily', 'Normal', 'Dry'], yticklabels=['Oily', 'Normal', 'Dry'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Menampilkan laporan klasifikasi\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=['Oily', 'Normal', 'Dry']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
