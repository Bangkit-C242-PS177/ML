{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Conditions Model 2\n",
    "This model is the model that used to classify user's skin conditions and have an output of multi-label class from 3 possible class (acne,eye_bags, redness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries, Mobilenet, and Env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Env File\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "datasets_path = os.getenv('DATASET_PATH_CONDITIONS_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing MobileNetV2 Model With ImagNet Weight Without The Top Layer\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membekukan semua lapisan dari model MobileNetV2\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning: Membuka beberapa lapisan terakhir dari MobileNetV2\n",
    "for layer in base_model.layers[-15:]:  # Mengatur lebih banyak lapisan terakhir dapat dilatih\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Trained Model (If Exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load model from .h5 file\n",
    "model = load_model('models/skin_conditions_model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset (Tanpa Data Augmentasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SkinConditionsDataset(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"DatasetBuilder for skin condition detection.\"\"\"\n",
    "\n",
    "    VERSION = tfds.core.Version('1.0.0')\n",
    "    MANUAL_DOWNLOAD_INSTRUCTIONS = \"Please ensure the skin type dataset is downloaded and located at the right path (look at env file)\"\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            description=(\"Dataset for skin conditions classification with 2 labels: acne n eye_bags\"),\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                'image': tfds.features.Image(shape=(224, 224, 3)),\n",
    "                'label': tfds.features.Tensor(shape=(3,), dtype=tf.float32),\n",
    "            }),\n",
    "            supervised_keys=('image', 'label'),\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        # Mengambil data dari path yang telah diinstruksikan\n",
    "        path = dl_manager.manual_dir\n",
    "        \n",
    "        # Membaca semua gambar dan label\n",
    "        all_data = []\n",
    "        for category in ['acne', 'eye_bags']:\n",
    "            category_path = os.path.join(path, category)\n",
    "            for filename in os.listdir(category_path):\n",
    "                all_data.append((filename, category))\n",
    "\n",
    "        # Membuat DataFrame\n",
    "        df = pd.DataFrame(all_data, columns=['filename', 'label'])\n",
    "\n",
    "        # Menyeimbangkan dataset dengan mengambil 500 sampel dari setiap kelas\n",
    "        # balanced_df = df.groupby('label').apply(lambda x: x.sample(n=500, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "        # Membagi data menjadi train, val, dan test\n",
    "        # train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 dari train untuk validasi\n",
    "\n",
    "        return {\n",
    "            'train': self._generate_examples(train_df, path),\n",
    "            'val': self._generate_examples(val_df, path),\n",
    "            'test': self._generate_examples(test_df, path),\n",
    "        }\n",
    "\n",
    "    def _generate_examples(self, dataframe, base_path):\n",
    "        \"\"\"Yields examples.\"\"\"\n",
    "        label_map = {\n",
    "            'acne': [1, 0],\n",
    "            'eye_bags': [0, 1],\n",
    "        }\n",
    "        for _, row in dataframe.iterrows():\n",
    "            image_path = os.path.join(base_path, row['label'], row['filename'])\n",
    "            unique_key = f\"{row['label']}_{row['filename']}\"  # Unique Key To Avoid Duplicates\n",
    "            print(f\"Accessing: {image_path}\")  # Debugging print statement\n",
    "            label = label_map[row['label']]\n",
    "            yield unique_key, {  # Using Unique Key\n",
    "                'image': image_path,\n",
    "                'label': label,\n",
    "            }\n",
    "\n",
    "\n",
    "builder = SkinConditionsDataset(data_dir=datasets_path)\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "class SkinConditionsDataset(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"DatasetBuilder for skin condition detection.\"\"\"\n",
    "\n",
    "    VERSION = tfds.core.Version('1.0.0')\n",
    "    MANUAL_DOWNLOAD_INSTRUCTIONS = \"Please ensure the skin type dataset is downloaded and located at the right path (look at env file)\"\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            description=(\"Dataset for skin conditions classification with 2 labels: acne n eye_bags\"),\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                'image': tfds.features.Image(shape=(224, 224, 3)),\n",
    "                'label': tfds.features.Tensor(shape=(2,), dtype=tf.float32),\n",
    "            }),\n",
    "            supervised_keys=('image', 'label'),\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        path = dl_manager.manual_dir\n",
    "        \n",
    "        # Membaca semua gambar dan label\n",
    "        all_data = []\n",
    "        for category in ['acne', 'eye_bags']:\n",
    "            category_path = os.path.join(path, category)\n",
    "            for filename in os.listdir(category_path):\n",
    "                all_data.append((filename, category))\n",
    "\n",
    "        # Membuat DataFrame\n",
    "        df = pd.DataFrame(all_data, columns=['filename', 'label'])\n",
    "\n",
    "        # Membagi data menjadi train, val, dan test\n",
    "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 dari train untuk validasi\n",
    "\n",
    "        return {\n",
    "            'train': self._generate_examples(train_df, path),\n",
    "            'val': self._generate_examples(val_df, path),\n",
    "            'test': self._generate_examples(test_df, path),\n",
    "        }\n",
    "\n",
    "    def _generate_examples(self, dataframe, base_path):\n",
    "        label_map = {\n",
    "            'acne': [1, 0],\n",
    "            'eye_bags': [0, 1],\n",
    "        }\n",
    "        images_labels = []\n",
    "\n",
    "        # Load images and their labels\n",
    "        for _, row in dataframe.iterrows():\n",
    "            image_path = os.path.join(base_path, row['label'], row['filename'])\n",
    "            image = tf.io.read_file(image_path)  # Membaca file gambar\n",
    "            image = tf.io.decode_image(image, channels=3)  # Decode gambar ke tensor\n",
    "            image = tf.image.resize(image, (224, 224))  # Ubah ukuran gambar\n",
    "            image = tf.cast(image, tf.uint8)  # Konversi tipe data ke uint8\n",
    "            label = tf.cast(label_map[row['label']], tf.float32)  # Cast to float32\n",
    "            images_labels.append((image, label))  # Tambahkan ke list\n",
    "\n",
    "        # Generate standard examples\n",
    "        for i, (image, label) in enumerate(images_labels):\n",
    "            yield f\"{label.numpy()}_{i}\", {  # Mengubah label menjadi NumPy array sebelum yield\n",
    "                'image': image.numpy(),\n",
    "                'label': label.numpy(),  # Pastikan label adalah NumPy array\n",
    "            }\n",
    "\n",
    "        # Generate overlay examples\n",
    "        for i in range(0, len(images_labels), 3):  # Ambil setiap 3 gambar\n",
    "            # Ambil salah satu gambar dari 3 gambar untuk di-overlay\n",
    "            selected_images = images_labels[i:i+3]  # Mengambil 3 gambar berturut-turut\n",
    "            if len(selected_images) < 3:\n",
    "                continue  # Skip jika tidak ada cukup gambar dalam kelompok\n",
    "            \n",
    "            # Pilih satu gambar acak untuk di-overlay dengan dua lainnya\n",
    "            random.shuffle(selected_images)  # Acak gambar dalam kelompok tiga\n",
    "            image1, label1 = selected_images[0]  # Gambar pertama yang dipilih\n",
    "            image2, label2 = selected_images[1]  # Gambar kedua yang dipilih\n",
    "\n",
    "            # Buat overlay hanya dari dua gambar yang dipilih\n",
    "            combined_image, combined_label = self.overlay_two_data(image1, label1, image2, label2)\n",
    "            combined_image = tf.cast(combined_image, tf.uint8)  # Pastikan konversi ke uint8 untuk overlay\n",
    "\n",
    "            # Buat key unik untuk overlay\n",
    "            unique_key = f\"overlay_{i}_{i+1}\"\n",
    "\n",
    "            # Yield contoh baru untuk dataset\n",
    "            yield unique_key, {\n",
    "                'image': combined_image.numpy(),\n",
    "                'label': combined_label.numpy(),  # Pastikan label adalah NumPy array\n",
    "            }\n",
    "\n",
    "    # Fungsi untuk menggabungkan dua gambar\n",
    "    def overlay_two_images(self, image1, image2, alpha=0.5):\n",
    "        # Resize kedua gambar ke ukuran yang sama\n",
    "        image1 = tf.image.resize(image1, (224, 224))\n",
    "        image2 = tf.image.resize(image2, (224, 224))\n",
    "\n",
    "        # Gabungkan gambar dengan alpha untuk mengatur seberapa banyak kontribusi dari setiap gambar\n",
    "        combined_image = (image1 * alpha) + (image2 * alpha)\n",
    "\n",
    "        return combined_image\n",
    "\n",
    "    # Fungsi untuk overlay dua data (label juga digabung)\n",
    "    def overlay_two_data(self, image1, label1, image2, label2):\n",
    "        combined_image = self.overlay_two_images(image1, image2)\n",
    "        # Gabungkan label dengan mengambil nilai maksimum untuk setiap kelas\n",
    "        combined_label = tf.maximum(label1, label2)\n",
    "        return combined_image, combined_label\n",
    "\n",
    "\n",
    "# Use the updated dataset class\n",
    "builder = SkinConditionsDataset(data_dir=datasets_path)\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset dalam bentuk builder untuk melakukan cek\n",
    "ds_train = builder.as_dataset(split='train')\n",
    "ds_val = builder.as_dataset(split='val')\n",
    "ds_test = builder.as_dataset(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dan tampilkan label dengan nilai [0, 1]\n",
    "for i, example in enumerate(ds_train):\n",
    "    label = example['label'].numpy()  # Ambil label sebagai numpy array\n",
    "    if (label == [1, 1]).all():    \n",
    "        print(f\"Label {i + 1}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Fungsi untuk menampilkan gambar\n",
    "def show_image(image):\n",
    "    # Konversi gambar tensor ke numpy array dan tampilkan dengan matplotlib\n",
    "    image = image.numpy()  # Ubah tensor menjadi numpy array\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Nonaktifkan axis\n",
    "    plt.show()\n",
    "\n",
    "# Fungsi untuk menampilkan beberapa gambar\n",
    "def show_images_from_dataset(dataset, num_images=5):\n",
    "    for i, data in enumerate(dataset.take(num_images)):  # Ambil beberapa gambar pertama dari dataset\n",
    "        image = data['image']\n",
    "        # Tampilkan gambar\n",
    "        show_image(image)\n",
    "\n",
    "# Menampilkan gambar pertama dari ds_train\n",
    "show_images_from_dataset(ds_train, num_images=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset yang sudah diproses\n",
    "ds_train = tfds.load('skin_conditions_dataset', split='train', data_dir=datasets_path)\n",
    "ds_val = tfds.load('skin_conditions_dataset', split='val', data_dir=datasets_path)\n",
    "ds_test = tfds.load('skin_conditions_dataset', split='test', data_dir=datasets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Fungsi untuk memastikan data tersusun sebagai pasangan (image, label)\n",
    "def preprocess(data):\n",
    "    image = data['image']\n",
    "    label = data['label']\n",
    "    return image, label\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = preprocess_input(image)  # Preprocessing sesuai MobileNet\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_val = ds_val.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_train = ds_train.map(preprocess_image)\n",
    "ds_val = ds_val.map(preprocess_image)\n",
    "ds_test = ds_test.map(preprocess_image)\n",
    "\n",
    "ds_train = ds_train.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung jumlah data dalam ds_train\n",
    "num_samples = sum(1 for _ in ds_train)\n",
    "# num_samples = sum(1 for _ in ds_train.unbatch())\n",
    "\n",
    "print(f\"Jumlah gambar dan label dalam ds_train: {num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, BatchNormalization, GlobalAveragePooling2D, Dense, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Menambahkan lapisan kustom di atas MobileNetV2\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) \n",
    "# x = Dense(1024, kernel_regularizer=l2(0.01))(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = ReLU()(x)  # ReLU activation setelah batch normalization\n",
    "# x = Dropout(0.4)(x) \n",
    "x = Dense(512, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)  # ReLU activation setelah batch normalization\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(2, activation='sigmoid', kernel_regularizer=l2(0.01))(x)  # Menggunakan sigmoid untuk multi-label classification\n",
    "\n",
    "# Membuat model akhir\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "# Menyesuaikan optimizer dan learning rate \n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9) \n",
    "# Compiling the model \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Model Architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Callback untuk menghentikan pelatihan jika validasi loss tidak membaik\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',                  \n",
    "    patience=5,                         \n",
    "    restore_best_weights=True,           \n",
    "    verbose=1                            \n",
    ")\n",
    "\n",
    "# Callback untuk mengurangi learning rate jika validasi loss stagnan\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',                \n",
    "    factor=0.5,                         \n",
    "    patience=3,                         \n",
    "    verbose=1                           \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in ds_train.take(1):\n",
    "    print(image.shape, label.shape)  # Pastikan gambar memiliki shape (32, 224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = {0: 0.85, 1: 1.2}  # Sesuaikan bobot berdasarkan kinerja kelas\n",
    "\n",
    "# Melatih model dan mencatat hasil pelatihan dalam objek `history`\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=30,\n",
    "    # class_weight=class_weight,\n",
    "    callbacks=[early_stopping, lr_scheduler] \n",
    ")\n",
    "\n",
    "# Menampilkan metrik dengan matplotlib\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(accuracy))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluasi model pada data testing\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f'Testing Loss: {loss}, Testing Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil nilai akurasi pada epoch terbaik\n",
    "best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "\n",
    "train_accuracy = history.history['accuracy'][best_epoch] \n",
    "val_accuracy = history.history['val_accuracy'][best_epoch]\n",
    "\n",
    "# Akurasi data testing (sudah didapat dari evaluasi model sebelumnya)\n",
    "test_accuracy = accuracy\n",
    "\n",
    "# Buat bar chart\n",
    "labels = ['Training Accuracy', 'Validation Accuracy', 'Testing Accuracy']\n",
    "accuracies = [train_accuracy, val_accuracy, test_accuracy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, accuracies, color=['blue', 'orange', 'green'])\n",
    "plt.ylim(0, 1)  # Atur batas y dari 0 sampai 1\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat prediksi pada data testing\n",
    "y_pred = model.predict(ds_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  # Mengubah prediksi menjadi biner\n",
    "\n",
    "# Mendapatkan label aktual\n",
    "y_true = np.concatenate([y for x, y in ds_test], axis=0)\n",
    "\n",
    "# Menghitung multilabel confusion matrix\n",
    "cm = multilabel_confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "# Menampilkan confusion matrix untuk masing-masing label\n",
    "labels = ['Acne', 'Eye Bags']\n",
    "for i, label in enumerate(labels):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm[i], annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix for {label}')\n",
    "    plt.show()\n",
    "\n",
    "# Menampilkan laporan klasifikasi\n",
    "print(classification_report(y_true, y_pred_binary, target_names=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in .h5 format\n",
    "model.save('models/skin_conditions_model_2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
